# **üìÑ Document QA com Arquitetura RAG (Retrieval-Augmented Generation)**

**Processamento de Linguagem Natural (PLN)** **Universidade da Beira Interior (UBI)** **Ano Letivo:** 2025/2026

## **üìã Sobre o Projeto**

Este reposit√≥rio cont√©m a implementa√ß√£o de um sistema de **Question Answering (QA)** sobre documentos n√£o estruturados, desenvolvido como trabalho final para a unidade curricular de Processamento de Linguagem Natural.

O objetivo principal foi criar uma aplica√ß√£o robusta capaz de ultrapassar as limita√ß√µes de contexto dos LLMs tradicionais, utilizando uma arquitetura **RAG (Retrieval-Augmented Generation)**. O sistema ingere documentos t√©cnicos longos (ex: relat√≥rios financeiros, manuais t√©cnicos, artigos cient√≠ficos), fragmenta-os e permite que o utilizador interaja com os mesmos atrav√©s de linguagem natural, com garantia de rastreabilidade da informa√ß√£o (cita√ß√£o de fontes).

### **üöÄ Funcionalidades Principais**

* **Ingest√£o de M√∫ltiplos Documentos:** Suporte para leitura e processamento em lote de ficheiros PDF.  
* **Vetoriza√ß√£o Persistente:** Cria√ß√£o de uma base de dados vetorial (Vector Store) local para evitar o reprocessamento de dados.  
* **LLM 100% Local:** Utiliza√ß√£o de modelos Open Source (Llama 3.2 ou Mistral) via Ollama, garantindo privacidade dos dados.  
* **Interface Interativa:** Aplica√ß√£o web desenvolvida em Streamlit com chat hist√≥rico e gest√£o de contexto.  
* **Preven√ß√£o de Alucina√ß√µes:** O sistema indica explicitamente o documento e o n√∫mero da p√°gina de onde a resposta foi extra√≠da.

## **üõ†Ô∏è Stack Tecnol√≥gica**

O projeto foi desenvolvido em **Python** utilizando as seguintes bibliotecas e ferramentas:

| Componente | Tecnologia | Descri√ß√£o |
| :---- | :---- | :---- |
| **LLM Server** | [Ollama](https://ollama.com/) | Execu√ß√£o local de modelos Llama/Mistral. |
| **Orquestra√ß√£o** | [LangChain](https://www.langchain.com/) | Framework para ligar o LLM aos dados. |
| **Vector Store** | [ChromaDB](https://www.trychroma.com/) | Base de dados para armazenamento de embeddings. |
| **Embeddings** | mxbai-embed-large | Modelo de vetoriza√ß√£o de texto de alto desempenho. |
| **Interface** | [Streamlit](https://streamlit.io/) | Frontend para intera√ß√£o com o utilizador. |
| **PDF Parsing** | pypdf | Extra√ß√£o de texto e metadados dos ficheiros. |

## **‚öôÔ∏è Instala√ß√£o e Configura√ß√£o**

### **1\. Pr√©-requisitos**

Certifique-se de que tem instalado:

* Python 3.8 ou superior.  
* [Ollama](https://ollama.com/) (para correr o modelo localmente).

### **2\. Configura√ß√£o do Modelo (Ollama)**

No seu terminal, descarregue os modelos necess√°rios:

\# Modelo de Linguagem (Chat)  
ollama pull llama3.2

\# Modelo de Embeddings (Vetoriza√ß√£o)  
ollama pull mxbai-embed-large

### **3\. Instala√ß√£o de Depend√™ncias**

Recomenda-se a cria√ß√£o de um ambiente virtual. Instale as bibliotecas Python necess√°rias:

pip install langchain langchain-community langchain-ollama langchain-chroma streamlit pypdf

## **üíª Como Utilizar**

O fluxo de trabalho divide-se em duas fases: **Ingest√£o de Dados** e **Intera√ß√£o**.

### **Passo 1: Prepara√ß√£o dos Dados (Ingest√£o)**

1. Coloque os seus ficheiros PDF na pasta data/.  
2. Execute o script de vetoriza√ß√£o. Este script ir√° ler os documentos, dividir o texto em fragmentos (chunks) e guardar os embeddings no ChromaDB.

python vector.py

*Nota: Execute este passo apenas na primeira vez ou sempre que adicionar novos documentos √† pasta data.*

### **Passo 2: Executar a Aplica√ß√£o**

Inicie a interface web Streamlit:

streamlit run app.py

A aplica√ß√£o ficar√° dispon√≠vel no seu navegador em http://localhost:8501.

## **üìÇ Estrutura do Reposit√≥rio**

.  
‚îú‚îÄ‚îÄ data/                     \# Diret√≥rio para colocar os ficheiros PDF de entrada  
‚îú‚îÄ‚îÄ chroma\_db\_financas/       \# Base de dados vetorial (gerada automaticamente)  
‚îú‚îÄ‚îÄ vector.py                 \# Script de pipeline ETL (Extract, Transform, Load)  
‚îú‚îÄ‚îÄ app.py                    \# Aplica√ß√£o principal (Frontend Streamlit \+ RAG Chain)  
‚îî‚îÄ‚îÄ README.md                 \# Documenta√ß√£o do projeto

## **üß© Detalhes de Implementa√ß√£o**

### **Estrat√©gia de Chunking**

Para lidar com documentos extensos, foi utilizado o RecursiveCharacterTextSplitter com:

* **Chunk Size:** 1500 caracteres (otimizado para capturar tabelas financeiras e par√°grafos completos).  
* **Overlap:** 300 caracteres (para manter o contexto sem√¢ntico entre cortes).

### **Recupera√ß√£o (Retrieval)**

Utiliza-se o algoritmo **MMR (Maximal Marginal Relevance)** na recupera√ß√£o de documentos para garantir diversidade nas fontes citadas e evitar que um √∫nico documento domine a janela de contexto do LLM.

## **üë§ Autor**

**Nome do Aluno** N√∫mero de Aluno: XXXXX

Engenharia Inform√°tica / Ci√™ncia de Dados

Universidade da Beira Interior